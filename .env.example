# AdGuard Home Configuration
AGH_URL=http://192.168.1.2:8080
AGH_USER=admin
AGH_PASS=password

# Application Configuration
POLL_INTERVAL=10s
DB_PATH=./data/guardian.db
LOG_LEVEL=info

# For Milestone 2+ (LLM Integration)
# LLM provider: gemini (recommended - free tier), ollama (local), openai, anthropic
LLM_PROVIDER=gemini

# Gemini Configuration (get free API key at https://aistudio.google.com/app/apikey)
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-1.5-flash

# Optional: Other LLM Providers
# OPENAI_API_KEY=
# ANTHROPIC_API_KEY=
# OLLAMA_URL=http://localhost:11434
# OLLAMA_MODEL=llama3

# LLM Settings
LLM_TIMEOUT=30s
LLM_ENABLE=true

# LLM Batch Processing (Rate Limiting)
# These settings control how domains are batched before sending to the LLM
# Optimized for Gemini 2.5 Flash-Lite (15 RPM, 250K TPM, 1K RPD)
LLM_BATCH_SIZE=10          # Number of domains to analyze per API call (lower = fewer tokens)
LLM_BATCH_TIMEOUT=90s      # Maximum time to wait before processing a partial batch
LLM_BATCH_DELAY=90s        # Minimum delay between batch requests (prevents rate limiting)

# IMPORTANT: If you're getting rate limited with Flash-Lite:
# The issue is likely TPM (tokens per minute), not RPM:
# - Reduce LLM_BATCH_SIZE to 5 (fewer domains = fewer tokens per request)
# - Increase LLM_BATCH_DELAY to 120s (space out requests more)
# - Consider switching to gemini-1.5-flash (may have higher TPM limits)
#
# For faster processing (if not hitting limits):
# - Increase LLM_BATCH_SIZE to 15-20
# - Decrease LLM_BATCH_DELAY to 60s
